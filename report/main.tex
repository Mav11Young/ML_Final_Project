%!TeX program = xelatex
\documentclass[12pt,hyperref,a4paper,UTF8]{ctexart}
\usepackage{SJTUReport}

%%-------------------------------正文开始---------------------------%%
\begin{document}

%%-----------------------封面--------------------%%
\cover

%%------------------摘要-------------%%
\begin{abstract}

本文针对 CS3339 机器学习课程的期末项目，完成了一个基于高维特征的图像分类任务。该任务涉及 100 个预定义类别，每个图像由 512 维提取好的特征向量表示。本项目旨在通过统计学习方法实现高效分类，并探索高维数据的处理技巧。

在数据处理阶段，本文首先进行了详尽的探索性数据分析（EDA），包括特征分布可视化、数据稀疏性分析以及类别平衡性检查。随后，应用了主成分分析（PCA）等降维技术以应对维数灾难，并探讨了降维对模型性能的影响。

在模型构建方面，本文实现并对比了三种不同的统计学习方法，包括两种非深度学习模型（如逻辑回归、支持向量机 SVM）和一种深度学习/集成学习模型（如多层感知机 MLP 或随机森林）。其中，重点对基础算法进行了基于 Numpy 的手动实现，以加深对模型原理的理解。

最后，通过 K 折交叉验证等评估手段进行了模型选择与超参数调优。实验结果表明，所选模型在测试集上取得了良好的分类准确率，体现了所用方法的鲁棒性与泛化能力。

\end{abstract}

\thispagestyle{empty} % 首页不显示页码

%%--------------------------目录页------------------------%%
\newpage
\tableofcontents

%%------------------------正文页从这里开始-------------------%
\newpage

\section{引言}
\subsection{项目背景}
本项任务源自一个匿名图像分类数据集。在现代计算机视觉任务中，特征提取通常由预训练的深度卷积神经网络（CNN）完成。本项目聚焦于统计学习算法的应用，因此直接使用了预先提取好的 512 维特征向量，而不是原始图像数据。

\subsection{任务目标}
任务的目标是训练统计学习模型，对包含 100 个类别的图像特征进行分类。训练集包含 19573 个样本，测试集包含 10000 个样本。评价指标为分类准确率（Accuracy）。

\section{探索性数据分析 (EDA)}
\subsection{特征分布分析}
通过对 512 维特征的统计分析（如图 \ref{fig:feature_stats} 所示），发现特征的均值主要集中在 0.15 至 0.35 之间，标准差分布在 0.3 至 0.6 之间。由于不同特征维度的量纲和数值分布存在差异，直接使用原始数据可能会导致某些大方差特征主导模型训练。因此，在后续处理中，必须对特征进行标准化（Z-score Normalization）处理。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/feature_stats_dist.png}
    \caption{特征均值与标准差分布图}
    \label{fig:feature_stats}
\end{figure}

\subsection{数据稀疏性分析}
经统计，特征矩阵中的零元素占比约为 15.73\%。这一结果表明数据具有一定的稀疏性，但尚未达到极度稀疏的程度。这种稀疏性可能源于特征提取过程中某些特征仅在特定类别中被激活。在模型选择上，线性模型（如逻辑回归、SVM）通常能够较好地处理此类具有一定稀疏性的高维数据。

\subsection{类别分布情况}
数据集包含 100 个类别，样本总数为 19573 个。如图 \ref{fig:class_dist} 所示，各类别样本数量并不完全均匀，最多的类别约有 400 个样本，而最少的约有 100 个。这种轻微的类别不平衡要求我们在划分训练集与验证集时采用分层抽样（Stratified Split）策略，以确保模型评估的客观性。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/class_distribution.png}
    \caption{100个类别的样本数量分布图}
    \label{fig:class_dist}
\end{figure}

\section{数据预处理与降维}
\subsection{数据清洗与标准化}
针对 EDA 的结论，我们首先对特征矩阵进行了标准化处理，使其每一维特征均满足均值为 0、方差为 1 的分布。这一步有效地消除了不同维度间的量纲影响，加速了梯度下降算法的收敛，并提升了 SVM 等距离敏感模型的性能。

\subsection{主成分分析 (PCA)}
为了应对“维数灾难”并提升计算效率，我们应用了手动实现的 PCA 算法对 512 维特征进行降维分析。如图 \ref{fig:pca_variance} 所示，累计解释方差贡献率曲线在前期上升非常迅速。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/pca_variance.png}
    \caption{PCA 累计解释方差贡献率曲线}
    \label{fig:pca_variance}
\end{figure}

实验结果显示，大约前 80 个主成分即可保留原始数据 95\% 以上的方差信息。这意味着原始特征空间中存在大量的冗余信息和噪声。通过将维度从 512 降至 100 左右，不仅可以显著减少模型参数量和训练时间，还能在一定程度上起到正则化的作用，缓解过拟合现象。

\section{模型构建与算法实现}
\subsection{模型一：逻辑回归 (Logistic Regression)}
介绍逻辑回归的基本原理、损失函数及其在多分类任务中的扩展（One-vs-Rest 或 Softmax）。说明基于 Numpy 的手动实现细节。

\subsection{模型二：支持向量机 (SVM)}
探讨核函数的选择（线性核、RBF 核等）以及惩罚参数 C 的作用。

\subsection{模型三：[请选择你的第三个模型]}
例如：随机森林、AdaBoost 或多层感知机。

\section{模型评估与选择}
\subsection{交叉验证 (Cross-Validation)}
说明如何划分训练集与验证集，以及 K 折交叉验证的具体配置。

\subsection{超参数调优}
介绍使用网格搜索（Grid Search）或随机搜索（Random Search）对模型参数（如正则化系数、学习率、核参数等）进行优化的过程。

\section{实验结果与分析}
\subsection{性能对比}
对比不同模型在验证集上的表现（准确率、F1 分数等）。

\subsection{过拟合分析}
讨论正则化技术对缓解过拟合的作用。

\subsection{Kaggle 提交结果}
记录在 Kaggle 平台上的最终得分及排名。

\section{结论}
总结本项目的主要工作、所面临的挑战以及未来的改进方向。

%%----------- 参考文献 -------------------%%
%在reference.bib文件中填写参考文献，此处自动生成

\reference

\end{document}